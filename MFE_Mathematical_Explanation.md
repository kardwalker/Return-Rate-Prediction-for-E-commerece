# Mahalanobis Feature Extraction (MFE) - Mathematical Formulation

## ğŸ“ **Mathematical Foundation**

### **Problem Setup**
Given:
- **X** âˆˆ â„^(nÃ—p): Sparse categorical feature matrix (n samples, p features after one-hot encoding)
- **y** âˆˆ â„^n: Target variable (continuous or binary)
- **d**: Desired output dimension (d << p)

**Goal**: Find projection matrix **W** âˆˆ â„^(pÃ—d) that creates dense features **Z** = **XW** âˆˆ â„^(nÃ—d)

---

## ğŸ¯ **Core MFE Objective Function**

### **Mahalanobis Distance Formulation**

For a batch of data, MFE minimizes the **generalized Mahalanobis distance**:

```
ğ”(W) = (ZÌ„ - Ä’)áµ€ Vâ»Â¹ (ZÌ„ - Ä’)
```

Where:
- **Z** = **XW** (projected features)
- **ZÌ„** = **Z**áµ€**y** (correlation between projected features and target)
- **Ä’** = ğ”¼[**Z**áµ€**y**] (expected correlation)
- **V** = Cov(**Z**, **y**) (covariance matrix)

---

## ğŸ§® **Detailed Mathematical Steps**

### **Step 1: Forward Projection**
```
Z = XW
```
- **X** âˆˆ â„^(bÃ—p): Batch of sparse categorical features
- **W** âˆˆ â„^(pÃ—d): Projection matrix to learn
- **Z** âˆˆ â„^(bÃ—d): Dense projected features

### **Step 2: Statistics Computation**

#### **Correlation Vector**
```
ZÌ„ = Záµ€y = âˆ‘áµ¢â‚Œâ‚áµ‡ záµ¢yáµ¢ âˆˆ â„áµˆ
```

#### **Expected Correlation**
```
Ä’ = (âˆ‘áµ¢â‚Œâ‚áµ‡ záµ¢)(âˆ‘áµ¢â‚Œâ‚áµ‡ yáµ¢) / b âˆˆ â„áµˆ
```

#### **Covariance Matrix**
```
V = (1/b) âˆ‘áµ¢â‚Œâ‚áµ‡ (záµ¢ - zÌ„)(záµ¢ - zÌ„)áµ€ Ã— ÏƒÂ²áµ§ + Î»I
```

Where:
- **zÌ„** = (1/b)âˆ‘záµ¢ (mean of projected features)
- **ÏƒÂ²áµ§** = Var(y) (target variance)
- **Î»I**: Ridge regularization term

### **Step 3: Mahalanobis Distance**
```
Î´ = ZÌ„ - Ä’ âˆˆ â„áµˆ
ğ” = Î´áµ€Vâ»Â¹Î´ âˆˆ â„
```

### **Step 4: Gradient Computation**

#### **Gradient of Î´ with respect to W**
```
âˆ‚Î´/âˆ‚W = âˆ‚ZÌ„/âˆ‚W - âˆ‚Ä’/âˆ‚W
```

Where:
```
âˆ‚ZÌ„/âˆ‚W = Xáµ€y âˆˆ â„áµ– (broadcasted to â„áµ–Ë£áµˆ)
âˆ‚Ä’/âˆ‚W = (Xáµ€ğŸ™)(âˆ‘yáµ¢)/b âˆˆ â„áµ– (broadcasted to â„áµ–Ë£áµˆ)
```

#### **Final Gradient**
```
âˆ‡Wğ” = (âˆ‚Î´/âˆ‚W) âŠ— (2Vâ»Â¹Î´)
```

Where âŠ— denotes outer product.

### **Step 5: Weight Update**
```
W â† W + Î·âˆ‡Wğ”
```
- **Î·**: Learning rate

---

## ğŸ’» **Code-to-Math Mapping**

### **Your Code Implementation:**

```python
# Step 1: Forward projection
X_projected = X_batch.dot(W)  # Z = XW

# Step 2: Statistics
Z = X_projected.T.dot(y_batch)  # ZÌ„ = Záµ€y
sum_y = y_batch.sum()          # âˆ‘yáµ¢
sum_X = X_projected.sum(axis=0) # âˆ‘záµ¢
E_Z = (sum_X * sum_y) / b      # Ä’

# Step 3: Covariance
X_centered = X_projected - sum_X / b  # záµ¢ - zÌ„
S_x = X_centered.T.dot(X_centered)    # âˆ‘(záµ¢ - zÌ„)(záµ¢ - zÌ„)áµ€
y_var = (sum_y2 / b - (sum_y / b) ** 2)  # ÏƒÂ²áµ§
V = S_x * y_var + ridge * np.eye(d)      # V

# Step 4: Mahalanobis distance
delta = Z - E_Z                # Î´
V_inv = np.linalg.inv(V)      # Vâ»Â¹
M_val = delta.T.dot(V_inv).dot(delta)  # ğ”

# Step 5: Gradient computation
grad_factor = 2.0 * V_inv.dot(delta)  # 2Vâ»Â¹Î´
dZ_dW = X_batch_T.dot(y_batch)        # âˆ‚ZÌ„/âˆ‚W
dE_dW = X_batch_T.dot(np.ones(b)) * sum_y / b  # âˆ‚Ä’/âˆ‚W
grad_delta = (dZ_dW - dE_dW)          # âˆ‚Î´/âˆ‚W
grad_W = np.outer(grad_delta, grad_factor)  # âˆ‡Wğ”

# Step 6: Update
W += lr * grad_W
```

---

## ğŸ¯ **Intuitive Interpretation**

### **What MFE Optimizes:**

1. **Maximize Correlation**: Makes projected features **Z** highly correlated with target **y**
2. **Minimize Variance**: Reduces unwanted noise in the projected space
3. **Preserve Information**: Keeps the most predictive categorical relationships

### **Mathematical Intuition:**

```
Mahalanobis Distance = (Correlation - Expected)áµ€ Ã— Precision Ã— (Correlation - Expected)
```

This measures how "surprising" the correlation is, accounting for the natural variance in the data.

---

## ğŸ“Š **Dimensionality Transformation**

### **Before MFE:**
```
X_categorical âˆˆ â„â¿Ë£áµ– (sparse, p â‰ˆ 1000-10000)
```

### **After MFE:**
```
Z = XW âˆˆ â„â¿Ë£áµˆ (dense, d â‰ˆ 10-50)
```

### **Information Preservation:**
The projection **W** is learned such that:
```
corr(Z, y) â‰ˆ corr(X_optimal_subset, y)
```

---

## ğŸ”¬ **Advanced Mathematical Properties**

### **1. Optimization Landscape**
MFE optimizes a non-convex objective, but uses stochastic gradient descent with mini-batches for scalability.

### **2. Regularization**
Ridge term **Î»I** ensures numerical stability:
```
V_regularized = V + Î»I
```

### **3. Convergence**
The algorithm converges when:
```
||âˆ‡Wğ”|| < Îµ
```

---

## ğŸš€ **Why MFE Works**

1. **Statistical Efficiency**: Uses second-order statistics (covariance) vs first-order (correlation)
2. **Dimensionality Reduction**: p >> d compression while preserving predictive power
3. **Categorical Specialization**: Designed for sparse, high-dimensional categorical data
4. **Target Awareness**: Unlike PCA, it uses target information during feature extraction

This mathematical framework shows how MFE transforms sparse categorical embeddings into dense, predictive features through learned linear projections optimized via Mahalanobis distance minimization! ğŸ¯
